---
title: "Advanced Sports Data Lesson 7: Modeling with logistic regression"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
description: >
  Now we try to predict things and get a probability of an outcome.
---
```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(tidymodels)
library(hoopR)
library(zoo)
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.completion=FALSE)
```
# Modeling and logistic regression

## The basics

One of the most common -- and seemingly least rigorous -- parts of sports journalism is the prediction. There are no shortage of people making predictions about who will win a game or a league. Sure they have a method -- looking at how a team is playing, looking at the players, consulting their gut -- but rarely ever do you hear of a sports pundit using a model. 

We're going to change that. Throughout this class, you'll learn how to use modeling to make predictions. Some of these methods will predict numeric values (like how many points will a team score based on certain inputs). Some will predict categorical values (W or L, Yes or No, All Star or Not).

There are lots of problems in the world where the answer is not a number but *a classification*: Did they win or lose? Did the player get drafted or no? Is this player a flight risk to transfer or not?

These are problems of classification and there are algorithms we can use to estimate the probability that X will be the outcome. How likely is it that this team with these stats will win this game?

Where this gets interesting is in the middle.

```{r load-tidyverse, exercise=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(hoopR)
library(zoo)

set.seed(1234)
```
```{r load-tidyverse-solution}
library(tidyverse)
library(tidymodels)
library(hoopR)
library(zoo)

set.seed(1234)
```
```{r load-tidyverse-check}
grade_this_code()
```

What we need to do here is get both sides of the game. We'll start with getting the box scores and cleaning them up a bit. We'll split the shooting columns into made and missed and turn everything into a number. 

```{r logit-load-data, message=FALSE, warning=FALSE}
teamgames <- load_mbb_team_box(seasons = 2015:2023) %>%
  separate(field_goals_made_field_goals_attempted, into = c("field_goals_made","field_goals_attempted")) %>%
  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c("three_point_field_goals_made","three_point_field_goals_attempted")) %>%
  separate(free_throws_made_free_throws_attempted, into = c("free_throws_made","free_throws_attempted")) %>%
  mutate_at(12:34, as.numeric)

teamstats <- teamgames %>% 
  mutate(
    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,
    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),
    ppp = team_score/possessions
  )

rollingteamstats <- teamstats %>% 
  group_by(team_short_display_name, season) %>%
  arrange(game_date) %>%
  mutate(
    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,
    team_rolling_ppp = rollmean(lag(ppp, n=1), k=5, align="right", fill=NA)
    ) %>% 
  ungroup() %>% 
  na.omit()

team_side <- rollingteamstats %>%
  select(
    game_id,
    team_id, 
    team_short_display_name, 
    opponent_id, 
    game_date, 
    season, 
    team_score, 
    team_rolling_ppp
    ) %>% 
  na.omit()

opponent_side <- team_side %>%
  select(-opponent_id) %>% 
  rename(
    opponent_id = team_id,
    opponent_short_display_name = team_short_display_name,
    opponent_score = team_score,
    opponent_rolling_ppp = team_rolling_ppp
  ) %>%
  mutate(opponent_id = as.numeric(opponent_id)
)

games <- team_side %>% inner_join(opponent_side)

games <- games %>% mutate(
  team_result = as.factor(case_when(
    team_score > opponent_score ~ "W",
    opponent_score > team_score ~ "L"
))) %>% na.omit()

modelgames <- games %>% 
  select(
    game_id, 
    game_date, 
    team_short_display_name, 
    opponent_short_display_name, 
    season, 
    team_rolling_ppp, 
    opponent_rolling_ppp, 
    team_result
    ) 

log_split <- initial_split(modelgames, prop = .8)
log_train <- training(log_split)
log_test <- testing(log_split)

log_mod <- 
  logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")

log_recipe <- 
  recipe(team_result ~ ., data = log_train) %>% 
  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = "ID") %>%
  step_normalize(all_predictors())

log_workflow <- 
  workflow() %>% 
  add_model(log_mod) %>% 
  add_recipe(log_recipe)

log_fit <- 
  log_workflow %>% 
  fit(data = log_train)

trainpredict <- log_fit %>% predict(new_data = log_train) %>%
  bind_cols(log_train)

trainpredict <- log_fit %>% predict(new_data = log_train, type="prob") %>%
  bind_cols(trainpredict)

testpredict <- log_fit %>% predict(new_data = log_test) %>%
  bind_cols(log_test)

testpredict <- log_fit %>% predict(new_data = log_test, type="prob") %>%
  bind_cols(testpredict)
```
```{r logit-load-data-exercise, exercise = TRUE}
teamgames <- load_mbb_team_box(seasons = 2015:2023) %>%
  separate(field_goals_made_field_goals_attempted, into = c("field_goals_made","field_goals_attempted")) %>%
  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c("three_point_field_goals_made","three_point_field_goals_attempted")) %>%
  separate(free_throws_made_free_throws_attempted, into = c("free_throws_made","free_throws_attempted")) %>%
  mutate_at(12:34, as.numeric)
```
```{r logit-load-data-exercise-solution}
teamgames <- load_mbb_team_box(seasons = 2015:2023) %>%
  separate(field_goals_made_field_goals_attempted, into = c("field_goals_made","field_goals_attempted")) %>%
  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c("three_point_field_goals_made","three_point_field_goals_attempted")) %>%
  separate(free_throws_made_free_throws_attempted, into = c("free_throws_made","free_throws_attempted")) %>%
  mutate_at(12:34, as.numeric)
```
```{r logit-load-data-exercise-check}
grade_this_code()
```

## Feature engineering

Feature engineering is the process of using what you know about something -- domain knowledge -- to find features in data that can be used in machine learning algorithms. Sports is a great place for this because not only do we know a lot because we follow the sport, but lots of other people are looking at this all the time. Creativity is good.

A number of basketball heads -- including Ken Pomeroy of KenPom fame -- have noticed that one of the predictors of the outcome of basketball games are possession metrics. How efficient are teams with the possessions they have? Can't score if you don't have the ball, so how good is a team at pushing the play and getting more possessions, giving themselves more chances to score?

One problem? Possessions aren't in typical metrics. They aren't usually tracked. But you can estimate them from typical box scores. The way to do that is like this:

`Possessions = Field Goal Attempts â€“ Offensive Rebounds + Turnovers + (0.475 * Free Throw Attempts)`

If you look at the data we already have, however, you'll see possessions are not actually in the data. Which is unfortunate. But we can calculate it pretty easily.

Then we'll use the possessions estimate formula to get that, so we can then calculate points per possession.

We'll save that to a new dataframe called `teamstats`.

### Exercise 1: setting up your data

```{r team-data, exercise=TRUE, exercise.setup = "logit-load-data"}
teamstats <- teamgames %>% 
  mutate(
    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,
    possessions = ?????_?????_attempted - offensive_rebounds + ????????? + (.475 * free_throws_attempted),
    ppp = team_score/possessions
  )
```
```{r team-data-solution, exercise.reveal_solution = FALSE}
teamstats <- teamgames %>% 
  mutate(
    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,
    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),
    ppp = team_score/possessions
  )
```
```{r team-data-check}
grade_this_code()
```

Now we begin the process of creating a model. Modeling in data science has a **ton** of details, but the process for each model type is similar.

1.  Split your data into training and testing data sets. A common split is 80/20.
2.  Train the model on the training dataset.
3.  Evaluate the model on the training data.
4.  Apply the model to the testing data.
5.  Evaluate the model on the test data.

From there, it's how you want to use the model. We'll walk through a simple example here, using a simple model -- a logistic regression model.

What we're trying to do here is predict which team will win given their efficiency with the ball, expressed as points per possession. However, to make a prediction, we need to know their stats *BEFORE* the game -- what we knew about the team going into the game in question. We can do that using `zoo` and rolling means. 

A rolling mean is an average in a window of time. So if we averaged together the points per possession over 10 games, that's a 10 game rolling mean. The first real mean would be games 1-10. Then the window would shift one game with game 11, and the average would be games 2-11. Then 3-12, 4-13 and so on. 

We'll add three new columns -- the one game lagged rolling mean of shooting percentage, points per possession and true shooting percentage. 

The problem we have to face here is that with our data, a rolling mean of games 6-15 would mean if we were trying to predict game 15, we couldn't *include* game 15. We'd have to look at games 5-14. If we included game 15, it would mean we had God like abilities to predict the future. 

We do not. Introducing the `lag` function. The lag function just takes the window of data and shifts it however many spots back you want to shift it. In our case, we want to shift it one game back. And we're going to make a rolling window of 5 games.

### Exercise 2: Lagging

```{r lag-data, exercise=TRUE, exercise.setup = "logit-load-data"}
rollingteamstats <- teamstats %>% 
  group_by(team_short_display_name, season) %>%
  arrange(game_date) %>%
  mutate(
    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,
    team_rolling_ppp = rollmean(???(ppp, n=?), k=5, align="right", fill=NA)
    ) %>% 
  ungroup() %>% 
  na.omit()
```
```{r lag-data-solution, exercise.reveal_solution = FALSE}
rollingteamstats <- teamstats %>% 
  group_by(team_short_display_name, season) %>%
  arrange(game_date) %>%
  mutate(
    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,
    team_rolling_ppp = rollmean(lag(ppp, n=1), k=5, align="right", fill=NA)
    ) %>% 
  ungroup() %>% 
  na.omit()
```
```{r lag-data-check}
grade_this_code()
```

Now we need to do something that at first will seem kind of odd, but isn't when you think about it. Our data has half of the box score -- just one team. But a game has two teams in it. To get it, we need the team AND the opponent. How can you decide if a team is going to win if you don't know who they are playing and if that team is any good or not? So we need to create two dataframes that have column names that indicate these stats are the team stats and these stats are the opponent stats. We can do that with some selecting and some renaming. 

```{r rename-data, exercise=TRUE, exercise.setup = "logit-load-data"}
team_side <- rollingteamstats %>%
  select(
    game_id,
    team_id, 
    team_short_display_name, 
    opponent_id, 
    game_date, 
    season, 
    team_score, 
    team_rolling_ppp
    ) %>% 
  na.omit()

opponent_side <- team_side %>%
  select(-opponent_id) %>% 
  rename(
    opponent_id = team_id,
    opponent_short_display_name = team_short_display_name,
    opponent_score = team_score,
    opponent_rolling_ppp = team_rolling_ppp
  ) %>%
  mutate(opponent_id = as.numeric(opponent_id)
)
```
```{r rename-data-solution, exercise.reveal_solution = FALSE}
team_side <- rollingteamstats %>%
  select(
    game_id,
    team_id, 
    team_short_display_name, 
    opponent_id, 
    game_date, 
    season, 
    team_score, 
    team_rolling_ppp
    ) %>% 
  na.omit()

opponent_side <- team_side %>%
  select(-opponent_id) %>% 
  rename(
    opponent_id = team_id,
    opponent_short_display_name = team_short_display_name,
    opponent_score = team_score,
    opponent_rolling_ppp = team_rolling_ppp
  ) %>%
  mutate(opponent_id = as.numeric(opponent_id)
)
```
```{r rename-data-check}
grade_this_code()
```

Now we'll join them together. 

### Exercise 3: Joining the data together

```{r join-data, exercise=TRUE, exercise.setup = "logit-load-data"}
games <- ????_side %>% inner_join(?????????_side)
```
```{r join-data-solution, exercise.reveal_solution = FALSE}
games <- team_side %>% inner_join(opponent_side)
```
```{r join-data-check}
grade_this_code()
```

The last problem to solve? Who won? We can add this with conditional logic. The other thing we're doing here is we're going to is we're going to convert our new team_result column into a factor. What is a factor? A factor is a type of data in R that stores categorical values that have a limited number of differences. So wins and losses are a perfect factor. Modeling libraries are looking for factors so it can treat the differences in the data as categories, so that's why we're converting it here. 

```{r wins, exercise=TRUE, exercise.setup = "logit-load-data"}
games <- games %>% mutate(
  team_result = as.factor(case_when(
    team_score > opponent_score ~ "W",
    opponent_score > team_score ~ "L"
))) %>% na.omit()
```
```{r wins-solution, exercise.reveal_solution = FALSE}
games <- games %>% mutate(
  team_result = as.factor(case_when(
    team_score > opponent_score ~ "W",
    opponent_score > team_score ~ "L"
))) %>% na.omit()
```
```{r wins-check}
grade_this_code()
```

Now that we've done that, we need to look at the order of our factors. 

### Exercise 4: Looking at the factors

```{r levels, exercise=TRUE, exercise.setup = "logit-load-data"}
levels(games$????_??????)
```
```{r levels-solution, exercise.reveal_solution = FALSE}
levels(games$team_result)
```
```{r levels-check}
grade_this_code()
```

The order listed here is the order they are in. What this means is that our predictions will be done through the lens of losses. That doesn't make intuitive sense to us. We want to know who will win! We can reorder the factors with `relevel`. 

### Exercise 5: Releveling the factors

```{r relevel, exercise=TRUE, exercise.setup = "logit-load-data"}
games$team_result <- relevel(games$team_result, ref="?")

levels(games$team_result)
```
```{r relevel-solution, exercise.reveal_solution = FALSE}
games$team_result <- relevel(games$team_result, ref="W")

levels(games$team_result)
```
```{r relevel-check}
grade_this_code()
```

For simplicity, let's limit the number of columns we're going to feed our model.

```{r selecting, exercise=TRUE, exercise.setup = "logit-load-data"}
modelgames <- games %>% 
  select(
    game_id, 
    game_date, 
    team_short_display_name, 
    opponent_short_display_name, 
    season, 
    team_rolling_ppp, 
    opponent_rolling_ppp, 
    team_result
    ) 
```
```{r selecting-solution, exercise.reveal_solution = FALSE}
modelgames <- games %>% 
  select(
    game_id, 
    game_date, 
    team_short_display_name, 
    opponent_short_display_name, 
    season, 
    team_rolling_ppp, 
    opponent_rolling_ppp, 
    team_result
    ) 
```
```{r selecting-check}
grade_this_code()
```

## Visualizing the decision boundary

This is just one dimension of the data, but it can illustrate how this works. You can almost see a line running through the middle, with a lot of overlap. The further left or right you go, the less overlap. You can read it like this: If this team shoots this well and the opponent shoots this well, most of the time this team wins. Or loses. It just depends on where the dot ends up. 

That neatly captures the probabilities we're looking at here. 

```{r boundarychart, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "logit-load-data"}
ggplot() + 
  geom_point(
    data=games, aes(x=team_rolling_ppp, y=opponent_rolling_ppp, color=team_result))
```

## The logistic regression

To create a model, we have to go through a process. That process starts with splitting data where we know the outomes into two groups -- training and testing. The training data is what we will use to create our model. The testing data is how we will determine how good it is. Then, going forward, our model can predict games we *haven't* seen yet. 

To do this, we're going to first split our `modelgames` data into two groups -- with 80 percent of it in one, 20 percent in the other. We do that by feeding our simplified dataframe into the `initial_split` function. Then we'll explicitly name those into new dataframes called train and test. 

### Exercise 6: What are we splitting?

```{r split, exercise=TRUE, exercise.setup = "logit-load-data"}
log_split <- initial_split(?????????, prop = .8)
log_train <- training(log_split)
log_test <- testing(log_split)
```
```{r split-solution, exercise.reveal_solution = FALSE}
log_split <- initial_split(modelgames, prop = .8)
log_train <- training(log_split)
log_test <- testing(log_split)
```
```{r split-check}
grade_this_code()
```

Now we have two dataframes -- log_train and log_test -- that we can now use for modeling.

First step to making a model is to set what type of model this will be. We're going to name our model object -- log_mod works because this is a logistic regression model. We'll use the logistic_reg function in parsnip (the modeling library in Tidymodels) and set the engine to "glm". The mode in our case is "classification" because we're trying to classify something as a W or L. Later, we'll use "regression" to predict numbers.

```{r model, exercise=TRUE, exercise.setup = "logit-load-data"}
log_mod <- 
  logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")
```
```{r model-solution, exercise.reveal_solution = FALSE}
log_mod <- 
  logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")
```
```{r model-check}
grade_this_code()
```

The next step is to create a recipe. This is a series of steps we'll use to put our data into our model. For example -- what is predicting what? And what aren't predictors and what are? And do we have to do any pre-processing of the data?

The first part of the recipe is the formula. In this case, we're saying -- in real words -- `team_result` *is approximately modeled by* our predictors, which we represent as `.` which means all the stuff. Then, importantly, we say what *isn't* a predictor next with update_role. So the team name, the game date and things like that are *not* predictors. So we need to tell it that. The last step is normalizing our numbers. With logistic regression, scale differences in numbers can skew things, so we're going to turn everything into Z-scores. 

```{r recipe, exercise=TRUE, exercise.setup = "logit-load-data"}
log_recipe <- 
  recipe(team_result ~ ., data = log_train) %>% 
  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = "ID") %>%
  step_normalize(all_predictors())

summary(log_recipe)
```
```{r recipe-solution, exercise.reveal_solution = FALSE}
log_recipe <- 
  recipe(team_result ~ ., data = log_train) %>% 
  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = "ID") %>%
  step_normalize(all_predictors())

summary(log_recipe)
```
```{r recipe-check}
grade_this_code()
```

Now we have enough for a workflow. A workflow is what we use to put it all together. In it, we add our model definition and our recipe. 

### Exercise 7: Making a workflow

```{r workflow, exercise=TRUE, exercise.setup = "logit-load-data"}
log_workflow <- 
  workflow() %>% 
  add_model(log_???) %>% 
  add_recipe(log_??????)
```
```{r workflow-solution, exercise.reveal_solution = FALSE}
log_workflow <- 
  workflow() %>% 
  add_model(log_mod) %>% 
  add_recipe(log_recipe)
```
```{r workflow-check}
grade_this_code()
```

And now we fit our model (this can take a few minutes).

```{r fit, exercise=TRUE, exercise.setup = "logit-load-data"}
log_fit <- 
  log_workflow %>% 
  fit(data = log_train)
```
```{r fit-solution, exercise.reveal_solution = FALSE}
log_fit <- 
  log_workflow %>% 
  fit(data = log_train)
```
```{r fit-check}
grade_this_code()
```

## Evaluating the fit

With logistic regression, there's two things we're looking at: The prediction and the probabilities. We can get those with two different fits and combine them together. 

First, you can see the predictions like this:

```{r predict1, exercise=TRUE, exercise.setup = "logit-load-data"}
trainpredict <- log_fit %>% predict(new_data = log_train) %>%
  bind_cols(log_train)

trainpredict
```
```{r predict1-solution, exercise.reveal_solution = FALSE}
trainpredict <- log_fit %>% predict(new_data = log_train) %>%
  bind_cols(log_train)

trainpredict
```
```{r predict1-check}
grade_this_code()
```

Then, we can just add it to `trainpredict` using `bind_cols`, which means we're going to bind the columns of this new fit to the old trainpredict.

```{r predict2, exercise=TRUE, exercise.setup = "logit-load-data"}
trainpredict <- log_fit %>% predict(new_data = log_train, type="prob") %>%
  bind_cols(trainpredict)

trainpredict
```
```{r predict2-solution, exercise.reveal_solution = FALSE}
trainpredict <- log_fit %>% predict(new_data = log_train, type="prob") %>%
  bind_cols(trainpredict)

trainpredict
```
```{r predict2-check}
grade_this_code()
```

You're probably going to see mulitple win and loss probabilities -- that's because of a quirk of how I'm doing this in learnR. You won't get that in your own notebooks. 

There's several metrics to look at to evaluate the model on our training data, but the two we will use are accuracy and roc_auc. They both are pointing toward how well the model did in two different ways. The accuracy metric looks at the number of predictions that are correct when compared to known results. The inputs here are the data, the column that has the actual result, and the column with the prediction, called `.pred_class`. 

### Exercise 8: Metrics

```{r accuracy, exercise=TRUE, exercise.setup = "logit-load-data"}
metrics(trainpredict, ????_??????, .pred_class)
```
```{r accuracy-solution, exercise.reveal_solution = FALSE}
metrics(trainpredict, team_result, .pred_class)
```
```{r accuracy-check}
grade_this_code()
```

So how accurate is our model? If we're looking for perfection, we're far from it. But if we're looking to make straight up win loss bets ... we're doing okay!

Another way to look at the results is the confusion matrix. The confusion matrix shows what was predicted compared to what actually happened. The squares are True Positives, False Positives, True Negatives and False Negatives. True values vs the total values make up the accuracy. 

### Exercise 9: Confusion matrix

```{r confusion, exercise=TRUE, exercise.setup = "logit-load-data"}
trainpredict %>%
  conf_mat(????_result, .pred_?????)
```
```{r confusion-solution, exercise.reveal_solution = FALSE}
trainpredict %>%
  conf_mat(team_result, .pred_class)
```
```{r confusion-check}
grade_this_code()
```

## Comparing it to test data

Now we can apply our fit to the test data to see how robust it is. If the metrics are similar, that's good -- it means our model is robust. If the metrics change a lot, that's bad. It means our model is guessing.

```{r test, exercise=TRUE, exercise.setup = "logit-load-data"}
testpredict <- log_fit %>% predict(new_data = log_test) %>%
  bind_cols(log_test)

testpredict <- log_fit %>% predict(new_data = log_test, type="prob") %>%
  bind_cols(testpredict)
```
```{r test-solution, exercise.reveal_solution = FALSE}
testpredict <- log_fit %>% predict(new_data = log_test) %>%
  bind_cols(log_test)

testpredict <- log_fit %>% predict(new_data = log_test, type="prob") %>%
  bind_cols(testpredict)
```
```{r test-check}
grade_this_code()
```

And now some metrics on the test data.

### Exercise 10: Testing

```{r testmetrics, exercise=TRUE, exercise.setup = "logit-load-data"}
metrics(????predict, team_result, .pred_class)
```
```{r testmetrics-solution, exercise.reveal_solution = FALSE}
metrics(testpredict, team_result, .pred_class)
```
```{r testmetrics-check}
grade_this_code()
```

How does that compare to our training data? Is it lower? Higher? Are the changes large -- like are we talking about single digit changes or double digit changes? The less it changes, the better. 

And now the confusion matrix.

```{r testconf, exercise=TRUE, exercise.setup = "logit-load-data"}
testpredict %>%
  conf_mat(team_result, .pred_class)
```
```{r testconf-solution, exercise.reveal_solution = FALSE}
testpredict %>%
  conf_mat(team_result, .pred_class)
```
```{r testconf-check}
grade_this_code()
```

How does that compare to the training data? 

## How well did it do with Nebraska?

Let's grab predictions for Nebraska from both our test and train data and take a look.

```{r nu1, exercise=TRUE, exercise.setup = "logit-load-data"}
nutrain <- trainpredict %>% filter(team_short_display_name == "Nebraska", season == 2023)

nutest <- testpredict %>% filter(team_short_display_name == "Nebraska", season == 2023)

bind_rows(nutrain, nutest) %>% 
  arrange(game_date) %>% 
  select(.pred_W, .pred_L, .pred_class, team_result, everything())
```
```{r nu1-solution, exercise.reveal_solution = FALSE}
nutrain <- trainpredict %>% filter(team_short_display_name == "Nebraska", season == 2023)

nutest <- testpredict %>% filter(team_short_display_name == "Nebraska", season == 2023)

bind_rows(nutrain, nutest) %>% 
  arrange(game_date) %>% 
  select(.pred_W, .pred_L, .pred_class, team_result, everything())
```
```{r nu1-check}
grade_this_code()
```

By our rolling metrics, are there any surprises? Should we have beaten Creighton or Iowa?

How could you improve this?
