---
title: "Lesson 8: Random forests"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
description: >
  Predicting things with thousands of decision trees.
---
```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(tidymodels)
library(hoopR)
library(zoo)
library(corrr)
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.completion=FALSE)
```
# Decision trees and random forests

## The basics

Tree-based algorithms are based on decision trees, which are very easy to understand. A decision tree can basically be described as a series of questions. Does this player have more or less than x seasons of experience? Do they have more or less then y minutes played? Do they play this or that position? Answer enough questions, and you can predict what that player should have on average.

The upside of decision trees is that if the model is small, you can explain it to anyone. They're very easy to understand. The trouble with decision trees is that if the model is small, they're a bit of a crude instrument. As such, multiple tree based methods have been developed as improvements on the humble decision tree.

The most common is the random forest.

Let's implement one. We start with libraries.

```{r load-tidyverse, exercise=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(hoopR)
library(zoo)
library(corrr)

set.seed(1234)
```
```{r load-tidyverse-solution}
library(tidyverse)
library(tidymodels)
library(hoopR)
library(zoo)
library(corrr)

set.seed(1234)
```
```{r load-tidyverse-check}
grade_this_code()
```

Let's use what we had from the last tutorial -- a rolling window of points per possession for team and opponent. I've gone ahead and run it all in the background. You can see modelgames by using `head` in the block.

```{r rf-load-data, message=FALSE, warning=FALSE}
teamgames <- load_mbb_team_box(seasons = 2015:2023) %>%
  separate(field_goals_made_field_goals_attempted, into = c("field_goals_made","field_goals_attempted")) %>%
  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c("three_point_field_goals_made","three_point_field_goals_attempted")) %>%
  separate(free_throws_made_free_throws_attempted, into = c("free_throws_made","free_throws_attempted")) %>%
  mutate_at(12:34, as.numeric)

teamstats <- teamgames %>% 
  mutate(
    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,
    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),
    ppp = team_score/possessions
  )

rollingteamstats <- teamstats %>% 
  group_by(team_short_display_name, season) %>%
  arrange(game_date) %>%
  mutate(
    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,
    team_rolling_ppp = rollmean(lag(ppp, n=1), k=5, align="right", fill=NA)
    ) %>% 
  ungroup() %>% 
  na.omit()

team_side <- rollingteamstats %>%
  select(
    game_id,
    team_id, 
    team_short_display_name, 
    opponent_id, 
    game_date, 
    season, 
    team_score, 
    team_rolling_ppp
    ) %>% 
  na.omit()

opponent_side <- team_side %>%
  select(-opponent_id) %>% 
  rename(
    opponent_id = team_id,
    opponent_short_display_name = team_short_display_name,
    opponent_score = team_score,
    opponent_rolling_ppp = team_rolling_ppp
  ) %>%
  mutate(opponent_id = as.numeric(opponent_id)
)

games <- team_side %>% inner_join(opponent_side)

games <- games %>% mutate(
  team_result = as.factor(case_when(
    team_score > opponent_score ~ "W",
    opponent_score > team_score ~ "L"
))) %>% na.omit()

modelgames <- games %>% 
  select(
    game_id, 
    game_date, 
    team_short_display_name, 
    opponent_short_display_name, 
    season, 
    team_rolling_ppp, 
    opponent_rolling_ppp, 
    team_result
    ) 
```
```{r rf-load-data-exercise, exercise = TRUE}
head(???????????)
```
```{r rf-load-data-exercise-solution}
head(modelgames)
```
```{r rf-load-data-exercise-check}
grade_this_code()
```

For this tutorial, we're going to create two models from two workflows so that we can compare a logistic regression to a random forest. 

## Setup



```{r split-data, exercise=TRUE, exercise.setup = "logit-load-data"}
game_split <- initial_split(modelgames, prop = .8)
game_train <- training(game_split)
game_test <- testing(game_split)
```
```{r split-data-solution, exercise.reveal_solution = FALSE}
game_split <- initial_split(modelgames, prop = .8)
game_train <- training(game_split)
game_test <- testing(game_split)
```
```{r split-data-check}
grade_this_code()
```

For this walkthrough, we're going to do both a logistic regression and a random forest side by side to show the value of workflows. 

The recipe we'll create is the same for both, so we'll use it twice. 

```{r recipe, exercise=TRUE, exercise.setup = "logit-load-data"}
game_recipe <- 
  recipe(team_result ~ ., data = game_train) %>% 
  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = "ID") %>%
  step_normalize(all_predictors())

summary(game_recipe)
```
```{r recipe-solution, exercise.reveal_solution = FALSE}
game_recipe <- 
  recipe(team_result ~ ., data = game_train) %>% 
  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = "ID") %>%
  step_normalize(all_predictors())

summary(game_recipe)
```
```{r recipe-check}
grade_this_code()
```

Now, we're going to create two different model specifications. The first will be the logistic regression model definintion and the second will be the random forest. 

```{r model, exercise=TRUE, exercise.setup = "logit-load-data"}
log_mod <- 
  logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")

rf_mod <- 
  rand_forest() %>% 
  set_engine("ranger") %>%
  set_mode("classification")
```
```{r model-solution, exercise.reveal_solution = FALSE}
log_mod <- 
  logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")

rf_mod <- 
  rand_forest() %>% 
  set_engine("ranger") %>%
  set_mode("classification")
```
```{r model-check}
grade_this_code()
```

Now we have enough for our workflows. We have two models and one recipe. 

```{r workflow, exercise=TRUE, exercise.setup = "logit-load-data"}
log_workflow <- 
  workflow() %>% 
  add_model(log_mod) %>% 
  add_recipe(game_recipe)

rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(game_recipe)
```
```{r workflow-solution, exercise.reveal_solution = FALSE}
log_workflow <- 
  workflow() %>% 
  add_model(log_mod) %>% 
  add_recipe(game_recipe)

rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(game_recipe)
```
```{r workflow-check}
grade_this_code()
```

Now we can fit our models to the data. 

```{r fits, exercise=TRUE, exercise.setup = "logit-load-data"}
log_fit <- 
  log_workflow %>% 
  fit(data = game_train)

rf_fit <- 
  rf_workflow %>% 
  fit(data = game_train)
```
```{r fits-solution, exercise.reveal_solution = FALSE}
log_fit <- 
  log_workflow %>% 
  fit(data = game_train)

rf_fit <- 
  rf_workflow %>% 
  fit(data = game_train)
```
```{r fits-check}
grade_this_code()
```

Now we can bind our predictions to the training data and see how we did.

```{r predict, exercise=TRUE, exercise.setup = "logit-load-data"}
logpredict <- log_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

logpredict <- log_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(logpredict)

rfpredict <- rf_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

rfpredict <- rf_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(rfpredict)
```
```{r predict-solution, exercise.reveal_solution = FALSE}
logpredict <- log_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

logpredict <- log_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(logpredict)

rfpredict <- rf_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

rfpredict <- rf_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(rfpredict)
```
```{r predict-check}
grade_this_code()
```

Now, how did we do? First, let's look at the logistic regression.

```{r metrics1, exercise=TRUE, exercise.setup = "logit-load-data"}
metrics(logpredict, team_result, .pred_class)
```
```{r metrics1-solution, exercise.reveal_solution = FALSE}
metrics(logpredict, team_result, .pred_class)
```
```{r metrics1-check}
grade_this_code()
```

Same as last time, the logistic regression model comes in at 62 percent accuracy, and when we expose it to testing data, it remains pretty stable. *This is a gigantic hint about what is to come.*

How about the random forest? 

```{r metrics2, exercise=TRUE, exercise.setup = "logit-load-data"}
metrics(rfpredict, team_result, .pred_class)
```
```{r metrics2-solution, exercise.reveal_solution = FALSE}
metrics(rfpredict, team_result, .pred_class)
```
```{r metrics2-check}
grade_this_code()
```

Holy buckets! We made a model that's 96 percent accurate? GET ME TO VEGAS.

Remember: Where a model makes its money is in data that it has never seen before. 

First, we look at logistic regression.

```{r test1, exercise=TRUE, exercise.setup = "logit-load-data"}
logtestpredict <- log_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

logtestpredict <- log_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(logtestpredict)

metrics(logtestpredict, team_result, .pred_class)
```
```{r test1-solution, exercise.reveal_solution = FALSE}
logtestpredict <- log_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

logtestpredict <- log_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(logtestpredict)

metrics(logtestpredict, team_result, .pred_class)
```
```{r test1-check}
grade_this_code()
```

Just about the same. That's a robust model.

Now, the inevitable crash with random forests. 
```{r test2, exercise=TRUE, exercise.setup = "logit-load-data"}
rftestpredict <- rf_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

rftestpredict <- rf_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(rftestpredict)

metrics(rftestpredict, team_result, .pred_class)
```
```{r test2-solution, exercise.reveal_solution = FALSE}
rftestpredict <- rf_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

rftestpredict <- rf_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(rftestpredict)

metrics(rftestpredict, team_result, .pred_class)
```
```{r test2-check}
grade_this_code()
```
Right at 57 percent. A little bit lower than logistic regression. But did they come to the same answers to get those numbers? No.

```{r confusion, exercise=TRUE, exercise.setup = "logit-load-data"}
logtestpredict %>%
  conf_mat(team_result, .pred_class)

rftestpredict %>%
  conf_mat(team_result, .pred_class)
```
```{r confusion-solution, exercise.reveal_solution = FALSE}
logtestpredict %>%
  conf_mat(team_result, .pred_class)

rftestpredict %>%
  conf_mat(team_result, .pred_class)
```
```{r confusion-check}
grade_this_code()
```

Our two models, based on our very basic feature engineering, are only slightly better than flipping a coin. If we want to get better, we've got work to do. 