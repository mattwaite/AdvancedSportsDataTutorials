---
title: "Advanced Sports Data Lesson 9: LightGBM"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
description: >
  An introduction to gradient boosting.
---
```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(tidymodels)
library(hoopR)
library(zoo)
library(bonsai)
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.completion=FALSE)
```
# LightGBM

## The basics

Let's implement one. We start with libraries.

```{r load-tidyverse, exercise=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(hoopR)
library(zoo)
library(bonsai)

set.seed(1234)
```
```{r load-tidyverse-solution}
library(tidyverse)
library(tidymodels)
library(hoopR)
library(zoo)
library(bonsai)

set.seed(1234)
```
```{r load-tidyverse-check}
grade_this_code()
```

Let's use what we had from the last tutorial -- a rolling window of points per possession for team and opponent. I've gone ahead and run it all in the background. You can see modelgames by using `head` in the block.

```{r rf-load-data, message=FALSE, warning=FALSE}
teamgames <- load_mbb_team_box(seasons = 2015:2023) %>%
  separate(field_goals_made_field_goals_attempted, into = c("field_goals_made","field_goals_attempted")) %>%
  separate(three_point_field_goals_made_three_point_field_goals_attempted, into = c("three_point_field_goals_made","three_point_field_goals_attempted")) %>%
  separate(free_throws_made_free_throws_attempted, into = c("free_throws_made","free_throws_attempted")) %>%
  mutate_at(12:34, as.numeric)

teamstats <- teamgames %>% 
  mutate(
    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,
    possessions = field_goals_attempted - offensive_rebounds + turnovers + (.475 * free_throws_attempted),
    ppp = team_score/possessions
  )

rollingteamstats <- teamstats %>% 
  group_by(team_short_display_name, season) %>%
  arrange(game_date) %>%
  mutate(
    team_score = ((field_goals_made-three_point_field_goals_made) * 2) + (three_point_field_goals_made*3) + free_throws_made,
    team_rolling_ppp = rollmean(lag(ppp, n=1), k=5, align="right", fill=NA)
    ) %>% 
  ungroup() %>% 
  na.omit()

team_side <- rollingteamstats %>%
  select(
    game_id,
    team_id, 
    team_short_display_name, 
    opponent_id, 
    game_date, 
    season, 
    team_score, 
    team_rolling_ppp
    ) %>% 
  na.omit()

opponent_side <- team_side %>%
  select(-opponent_id) %>% 
  rename(
    opponent_id = team_id,
    opponent_short_display_name = team_short_display_name,
    opponent_score = team_score,
    opponent_rolling_ppp = team_rolling_ppp
  ) %>%
  mutate(opponent_id = as.numeric(opponent_id)
)

games <- team_side %>% inner_join(opponent_side)

games <- games %>% mutate(
  team_result = as.factor(case_when(
    team_score > opponent_score ~ "W",
    opponent_score > team_score ~ "L"
))) %>% na.omit()

modelgames <- games %>% 
  select(
    game_id, 
    game_date, 
    team_short_display_name, 
    opponent_short_display_name, 
    season, 
    team_rolling_ppp, 
    opponent_rolling_ppp, 
    team_result
    ) 

game_split <- initial_split(modelgames, prop = .8)
game_train <- training(game_split)
game_test <- testing(game_split)

game_recipe <- 
  recipe(team_result ~ ., data = game_train) %>% 
  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = "ID") %>%
  step_normalize(all_predictors())

log_mod <- 
  logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")

rf_mod <- 
  rand_forest() %>% 
  set_engine("ranger") %>%
  set_mode("classification")

lightgbm_mod <- 
  boost_tree() %>%
  set_engine("lightgbm") %>%
  set_mode(mode = "classification")

log_workflow <- 
  workflow() %>% 
  add_model(log_mod) %>% 
  add_recipe(game_recipe)

rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(game_recipe)

lightgbm_workflow <- 
  workflow() %>% 
  add_model(lightgbm_mod) %>% 
  add_recipe(game_recipe)

log_fit <- 
  log_workflow %>% 
  fit(data = game_train)

rf_fit <- 
  rf_workflow %>% 
  fit(data = game_train)

lightgbm_fit <- 
  lightgbm_workflow %>% 
  fit(data = game_train)

logpredict <- log_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

logpredict <- log_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(logpredict)

rfpredict <- rf_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

rfpredict <- rf_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(rfpredict)

lightgbmpredict <- lightgbm_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

lightgbmpredict <- lightgbm_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(lightgbmpredict)

logtestpredict <- log_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

logtestpredict <- log_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(logtestpredict)

rftestpredict <- rf_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

rftestpredict <- rf_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(rftestpredict)

lightgbmtestpredict <- lightgbm_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

lightgbmtestpredict <- lightgbm_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(lightgbmtestpredict)
```

For this tutorial, we're going to create three models from three workflows so that we can compare a logistic regression to a random forest to a lightbgm model. 

## Setup

LIGHTGBM HERE

We're going to go through the steps of modeling again, starting with splitting our `modelgames` data.

### Exercise 1: setting up your data

```{r split-data, exercise=TRUE, exercise.setup = "rf-load-data"}
game_split <- initial_split(??????????, prop = .8)
game_train <- training(game_split)
game_test <- testing(game_split)
```
```{r split-data-solution, exercise.reveal_solution = FALSE}
game_split <- initial_split(modelgames, prop = .8)
game_train <- training(game_split)
game_test <- testing(game_split)
```
```{r split-data-check}
grade_this_code()
```

The recipe we'll create is the same for both, so we'll use it three times. 

### Exercise 2: setting up the receipe

So what data are we feeding into our recipe? 

```{r recipe, exercise=TRUE, exercise.setup = "rf-load-data"}
game_recipe <- 
  recipe(team_result ~ ., data = game_?????) %>% 
  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = "ID") %>%
  step_normalize(all_predictors())

summary(game_recipe)
```
```{r recipe-solution, exercise.reveal_solution = FALSE}
game_recipe <- 
  recipe(team_result ~ ., data = game_train) %>% 
  update_role(game_id, game_date, team_short_display_name, opponent_short_display_name, season, new_role = "ID") %>%
  step_normalize(all_predictors())

summary(game_recipe)
```
```{r recipe-check}
grade_this_code()
```

Now, we're going to create two different model specifications. The first will be the logistic regression model definintion and the second will be the random forest. 

```{r model, exercise=TRUE, exercise.setup = "rf-load-data"}
log_mod <- 
  logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")

rf_mod <- 
  rand_forest() %>% 
  set_engine("ranger") %>%
  set_mode("classification")

lightgbm_mod <- 
  boost_tree() %>%
  set_engine("lightgbm") %>%
  set_mode(mode = "classification")
```
```{r model-solution, exercise.reveal_solution = FALSE}
log_mod <- 
  logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")

rf_mod <- 
  rand_forest() %>% 
  set_engine("ranger") %>%
  set_mode("classification")

lightgbm_mod <- 
  boost_tree() %>%
  set_engine("lightgbm") %>%
  set_mode(mode = "classification")
```
```{r model-check}
grade_this_code()
```

Now we have enough for our workflows. We have two models and one recipe.

### Exercise 3: making workflows

```{r workflow, exercise=TRUE, exercise.setup = "rf-load-data"}
log_workflow <- 
  workflow() %>% 
  add_model(???_mod) %>% 
  add_recipe(????_recipe)

rf_workflow <- 
  workflow() %>% 
  add_model(??_mod) %>% 
  add_recipe(????_recipe)

lightgbm_workflow <- 
  workflow() %>% 
  add_model(light???_mod) %>% 
  add_recipe(game_recipe)
```
```{r workflow-solution, exercise.reveal_solution = FALSE}
log_workflow <- 
  workflow() %>% 
  add_model(log_mod) %>% 
  add_recipe(game_recipe)

rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(game_recipe)

lightgbm_workflow <- 
  workflow() %>% 
  add_model(lightgbm_mod) %>% 
  add_recipe(game_recipe)
```
```{r workflow-check}
grade_this_code()
```

Now we can fit our models to the data. 

### Exercise 4: fitting our models

```{r fits, exercise=TRUE, exercise.setup = "rf-load-data"}
log_fit <- 
  log_workflow %>% 
  fit(data = ????_?????)

rf_fit <- 
  rf_workflow %>% 
  fit(data = ????_?????)

lightgbm_fit <- 
  lightgbm_workflow %>% 
  fit(data = ????_?????)
```
```{r fits-solution, exercise.reveal_solution = FALSE}
log_fit <- 
  log_workflow %>% 
  fit(data = game_train)

rf_fit <- 
  rf_workflow %>% 
  fit(data = game_train)

lightgbm_fit <- 
  lightgbm_workflow %>% 
  fit(data = game_train)
```
```{r fits-check}
grade_this_code()
```

Now we can bind our predictions to the training data and see how we did.

```{r predict, exercise=TRUE, exercise.setup = "rf-load-data"}
logpredict <- log_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

logpredict <- log_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(logpredict)

rfpredict <- rf_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

rfpredict <- rf_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(rfpredict)

lightgbmpredict <- lightgbm_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

lightgbmpredict <- lightgbm_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(lightgbmpredict)
```
```{r predict-solution, exercise.reveal_solution = FALSE}
logpredict <- log_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

logpredict <- log_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(logpredict)

rfpredict <- rf_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

rfpredict <- rf_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(rfpredict)

lightgbmpredict <- lightgbm_fit %>% predict(new_data = game_train) %>%
  bind_cols(game_train) 

lightgbmpredict <- lightgbm_fit %>% predict(new_data = game_train, type="prob") %>%
  bind_cols(lightgbmpredict)
```
```{r predict-check}
grade_this_code()
```

Now, how did we do?

### Exercise 5: The first metrics

What prediction dataset do we feed into our metrics? Let's look first at the random forest.

```{r metrics1, exercise=TRUE, exercise.setup = "rf-load-data"}
metrics(?????????, team_result, .pred_class)
```
```{r metrics1-solution, exercise.reveal_solution = FALSE}
metrics(rfpredict, team_result, .pred_class)
```
```{r metrics1-check}
grade_this_code()
```

Same as last time, the random forest produces bonkers training numbers. Can you say overfit?

How about the lightgbm? 

### Exercise 6: LightGBM metrics

```{r metrics2, exercise=TRUE, exercise.setup = "rf-load-data"}
metrics(????????predict, team_result, .pred_class)
```
```{r metrics2-solution, exercise.reveal_solution = FALSE}
metrics(lightgbmpredict, team_result, .pred_class)
```
```{r metrics2-check}
grade_this_code()
```

About 63 percent accuracy. Which, if you'll recall, is about 3 percentage points better than logistic regression, and worse than random forest *WITH A HUGE ASTERISK.*

Remember: Where a model makes its money is in data that it has never seen before. 

First, we look at logistic regression.

```{r test1, exercise=TRUE, exercise.setup = "rf-load-data"}
logtestpredict <- log_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

logtestpredict <- log_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(logtestpredict)

metrics(logtestpredict, team_result, .pred_class)
```
```{r test1-solution, exercise.reveal_solution = FALSE}
logtestpredict <- log_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

logtestpredict <- log_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(logtestpredict)

metrics(logtestpredict, team_result, .pred_class)
```
```{r test1-check}
grade_this_code()
```

Just about the same. That's a robust model.

Now, the inevitable crash with random forests. 
```{r test2, exercise=TRUE, exercise.setup = "rf-load-data"}
rftestpredict <- rf_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

rftestpredict <- rf_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(rftestpredict)

metrics(rftestpredict, team_result, .pred_class)
```
```{r test2-solution, exercise.reveal_solution = FALSE}
rftestpredict <- rf_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

rftestpredict <- rf_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(rftestpredict)

metrics(rftestpredict, team_result, .pred_class)
```
```{r test2-check}
grade_this_code()
```
Right at 57 percent. A little bit lower than logistic regression. But did they come to the same answers to get those numbers? No.

And now lightGBM.

```{r test3, exercise=TRUE, exercise.setup = "rf-load-data"}
lightgbmtestpredict <- lightgbm_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

lightgbmtestpredict <- lightgbm_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(lightgbmtestpredict)

metrics(lightgbmtestpredict, team_result, .pred_class)
```
```{r test3-solution, exercise.reveal_solution = FALSE}
lightgbmtestpredict <- lightgbm_fit %>% predict(new_data = game_test) %>%
  bind_cols(game_test)

lightgbmtestpredict <- lightgbm_fit %>% predict(new_data = game_test, type="prob") %>%
  bind_cols(lightgbmtestpredict)

metrics(lightgbmtestpredict, team_result, .pred_class)
```
```{r test3-check}
grade_this_code()
```

Our three models, based on our very basic feature engineering, are *still* only slightly better than flipping a coin. If we want to get better, we've got work to do. 